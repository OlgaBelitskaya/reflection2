{"nbformat_minor": 0, "metadata": {"language_info": {"mimetype": "text/x-python", "version": "2.7.13", "file_extension": ".py", "nbconvert_exporter": "python", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}, "name": "python"}, "kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}}, "cells": [{"source": "# &#x1F4D1; &nbsp; <span style=\"color:#338DD4\"> Reflections. Machine Learning for Trading. Lessons 3<\/span>", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "##  &#x1F4CA; &nbsp; Links", "metadata": {}, "cell_type": "markdown"}, {"source": "A Tour of Machine Learning Algorithms: http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "Deep Learning for Multivariate Financial Time Series: http://www.math.kth.se/matstat/seminarier/reports/M-exjobb15/150612a.pdf", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "Parametric and Nonparametric Machine Learning Algorithms: http://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "Nearest Neighbors: http://scikit-learn.org/stable/modules/neighbors.html", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "Kernel ridge regression: http://scikit-learn.org/stable/modules/kernel_ridge.html", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "Cross-validation: evaluating estimator performance: http://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics", "metadata": {}, "cell_type": "markdown"}, {"source": "Cross-validation for time series: https://www.r-bloggers.com/cross-validation-for-time-series/", "metadata": {}, "cell_type": "markdown"}, {"source": "Ensemble methods: http://scikit-learn.org/stable/modules/ensemble.html", "metadata": {}, "cell_type": "markdown"}, {"source": "How to Build an Ensemble Of Machine Learning Algorithms in R (ready to use boosting, bagging and stacking): http://machinelearningmastery.com/machine-learning-ensembles-with-r/", "metadata": {}, "cell_type": "markdown"}, {"source": "Ensemble Machine Learning Algorithms in Python with scikit-learn: http://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/", "metadata": {}, "cell_type": "markdown"}, {"source": "Reinforcement Learning Toolkit: http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/RLtoolkit/RLtoolkit1.0.html", "metadata": {}, "cell_type": "markdown"}, {"source": "Deep Learning - The Past, Present and Future of Artificial Intelligence: https://www.slideshare.net/LuMa921/deep-learning-the-past-present-and-future-of-artificial-intelligence", "metadata": {}, "cell_type": "markdown"}, {"source": "Reinforcement Learning in Online Stock Trading Systems: https://pdfs.semanticscholar.org/be8e/61fe568712c799219fb612d190b4e62642ae.pdf", "metadata": {}, "cell_type": "markdown"}, {"source": "Creating a Planning and Learning Algorithm: http://burlap.cs.brown.edu/tutorials/cpl/p3.html", "metadata": {}, "cell_type": "markdown"}, {"source": "UC Berkeley CS188 Project 3 Reinforcement Learning: http://ai.berkeley.edu/reinforcement.html", "metadata": {}, "cell_type": "markdown"}, {"source": "Integrating Planning, Acting, and Learning (Dyna): https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node96.html", "metadata": {}, "cell_type": "markdown"}, {"source": "Q-Learning Step-By-Step Tutorial: http://mnemstudio.org/path-finding-q-learning-tutorial.htm", "metadata": {}, "cell_type": "markdown"}, {"source": "UCL Course on RL: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html", "metadata": {}, "cell_type": "markdown"}, {"source": "MathQuill: http://mathquill.com/", "metadata": {}, "cell_type": "markdown"}, {"source": "### 03-01 How Machine Learning is used at a hedge fund", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "Examples of good inputs (predictive factors) are:\n    \n1. Price momentum\n2. Bollinger value\n3. Current price\n\nwhile examples of outputs would be:\n    \n1. Future price\n2. Future return\n", "metadata": {}, "cell_type": "markdown"}, {"source": "### Algorithms\n#### By Learning Style:\n\n - ***Supervised Learning ( Labeled Data \u00e2\u0087\u0092 Direct feedback \u00e2\u0087\u0092 Predict outcome/future )***\n \nInput data is called training data and has a known label or result such as spam/not-spam or a stock price at a time.\n\nA model is prepared through a training process in which it is required to make predictions and is corrected when those predictions are wrong. The training process continues until the model achieves a desired level of accuracy on the training data.\n\nExample problems are classification and regression.\n\nExample algorithms include Logistic Regression and the Back Propagation Neural Network.\n\n- ***Unsupervised Learning ( No labels \u00e2\u0087\u0092 No feedback \u00e2\u0087\u0092 Find hidden structure )***\n\nInput data is not labeled and does not have a known result.\n\nA model is prepared by deducing structures present in the input data. This may be to extract general rules. It may be through a mathematical process to systematically reduce redundancy, or it may be to organize data by similarity.\n\nExample problems are clustering, dimensionality reduction and association rule learning.\n\nExample algorithms include: the Apriori algorithm and k-Means.\n\n- ***Semi-Supervised (Mixed) Learning***\n\nInput data is a mixture of labeled and unlabelled examples.\n\nThere is a desired prediction problem but the model must learn the structures to organize the data as well as make predictions.\n\nExample problems are classification and regression.\n\nExample algorithms are extensions to other flexible methods that make assumptions about how to model the unlabeled data.\n\n- ***Reinforcement Learning ( Reward System \u00e2\u0087\u0092 Decision Process \u00e2\u0087\u0092 Learn series of action )***\n\nInput data is a set of states, actions and a reward function which is affected by states and actions.\n\nA model is trained to find actions with maximum values of the reward function. \n\nExample problems are game playing and control problems. Canonical Example: Grid World.\n\nExample algorithms are Markov Decision Process, Temporal Difference (TD) learning, Q-learning, etc.", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "#### By Similarity:\n\n- ***Regression Algorithms***\n\nRegression is concerned with modeling the relationship between variables that is iteratively refined using a measure of error in the predictions made by the model.\n\nRegression methods are a workhorse of statistics and have been co-opted into statistical machine learning. This may be confusing because we can use regression to refer to the class of problem and the class of algorithm. Really, regression is a process.\n\nThe most popular regression algorithms are:\n\n      - Ordinary Least Squares Regression (OLSR)\n      - Linear Regression\n      - Logistic Regression\n      - Stepwise Regression\n      - Multivariate Adaptive Regression Splines (MARS)\n      - Locally Estimated Scatterplot Smoothing (LOESS)    \n\n\n - ***Instance-based Algorithms***\n\nInstance-based learning model is a decision problem with instances or examples of training data that are deemed important or required to the model.\n\nSuch methods typically build up a database of example data and compare new data to the database using a similarity measure in order to find the best match and make a prediction. For this reason, instance-based methods are also called winner-take-all methods and memory-based learning. Focus is put on the representation of the stored instances and similarity measures used between instances.\n\nThe most popular instance-based algorithms are:\n\n      - k-Nearest Neighbor (kNN)\n      - Learning Vector Quantization (LVQ)\n      - Self-Organizing Map (SOM)\n      - Locally Weighted Learning (LWL)\n   \n- ***Regularization Algorithms***\n\nAn extension made to another method (typically regression methods) that penalizes models based on their complexity, favoring simpler models that are also better at generalizing.\n\nI have listed regularization algorithms separately here because they are popular, powerful and generally simple modifications made to other methods.\n\nThe most popular regularization algorithms are:\n\n      - Ridge Regression\n      - Least Absolute Shrinkage and Selection Operator (LASSO)\n      - Elastic Net\n      - Least-Angle Regression (LARS)  \n\n\n- ***Decision Tree Algorithms***\n\nDecision tree methods construct a model of decisions made based on actual values of attributes in the data.\n\nDecisions fork in tree structures until a prediction decision is made for a given record. Decision trees are trained on data for classification and regression problems. Decision trees are often fast and accurate and a big favorite in machine learning.\n\nThe most popular decision tree algorithms are:\n\n      - Classification and Regression Tree (CART)\n      - Iterative Dichotomiser 3 (ID3)\n      - C4.5 and C5.0 (different versions of a powerful approach)\n      - Chi-squared Automatic Interaction Detection (CHAID)\n      - Decision Stump\n      - M5\n      - Conditional Decision Trees\n      \n- ***Bayesian Algorithms***\n\nBayesian methods are those that explicitly apply Bayes\u00e2\u0080\u0099 Theorem for problems such as classification and regression.\n\nThe most popular Bayesian algorithms are:\n\n      - Naive Bayes\n      - Gaussian Naive Bayes\n      - Multinomial Naive Bayes\n      - Averaged One-Dependence Estimators (AODE)\n      - Bayesian Belief Network (BBN)\n      - Bayesian Network (BN)\n      \n- ***Clustering Algorithms***\n\nClustering, like regression, describes the class of problem and the class of methods.\n\nClustering methods are typically organized by the modeling approaches such as centroid-based and hierarchal. All methods are concerned with using the inherent structures in the data to best organize the data into groups of maximum commonality.\n\nThe most popular clustering algorithms are:\n\n      - k-Means\n      - k-Medians\n      - Expectation Maximisation (EM)\n      - Hierarchical Clustering\n      \n- ***Association Rule Learning Algorithms***\n\nAssociation rule learning methods extract rules that best explain observed relationships between variables in data.\n\nThese rules can discover important and commercially useful associations in large multidimensional datasets that can be exploited by an organization.\n\nThe most popular association rule learning algorithms are:\n\n      - Apriori algorithm\n      - Eclat algorithm\n\n- ***Artificial Neural Network Algorithms***\n\nArtificial Neural Networks are models that are inspired by the structure and/or function of biological neural networks.\n\nThey are a class of pattern matching that are commonly used for regression and classification problems but are really an enormous subfield comprised of hundreds of algorithms and variations for all manner of problem types.\n\nNote that I have separated out Deep Learning from neural networks because of the massive growth and popularity in the field. Here we are concerned with the more classical methods.\n\nThe most popular artificial neural network algorithms are:\n\n      - Perceptron\n      - Back-Propagation\n      - Hopfield Network\n      - Radial Basis Function Network (RBFN)\n      \n- ***Deep Learning Algorithms***\n\nDeep Learning methods are a modern update to Artificial Neural Networks that exploit abundant cheap computation.\n\nThey are concerned with building much larger and more complex neural networks and, as commented on above, many methods are concerned with semi-supervised learning problems where large datasets contain very little labeled data.\n\nThe most popular deep learning algorithms are:\n\n      - Deep Boltzmann Machine (DBM)\n      - Deep Belief Networks (DBN)\n      - Convolutional Neural Network (CNN)\n      - Stacked Auto-Encoders\n          \n- ***Dimensionality Reduction Algorithms***\n\nLike clustering methods, dimensionality reduction seek and exploit the inherent structure in the data, but in this case in an unsupervised manner or order to summarize or describe data using less information.\n\nThis can be useful to visualize dimensional data or to simplify data which can then be used in a supervised learning method. Many of these methods can be adapted for use in classification and regression.\n\n      - Principal Component Analysis (PCA)\n      - Principal Component Regression (PCR)\n      - Partial Least Squares Regression (PLSR)\n      - Sammon Mapping\n      - Multidimensional Scaling (MDS)\n      - Projection Pursuit\n      - Linear Discriminant Analysis (LDA)\n      - Mixture Discriminant Analysis (MDA)\n      - Quadratic Discriminant Analysis (QDA)\n      - Flexible Discriminant Analysis (FDA)\n\n- ***Ensemble Algorithms***\n\nEnsemble methods are models composed of multiple weaker models that are independently trained and whose predictions are combined in some way to make the overall prediction.\n\nMuch effort is put into what types of weak learners to combine and the ways in which to combine them. This is a very powerful class of techniques and as such is very popular.\n\n      - Boosting\n      - Bootstrapped Aggregation (Bagging)\n      - AdaBoost\n      - Stacked Generalization (blending)\n      - Gradient Boosting Machines (GBM)\n      - Gradient Boosted Regression Trees (GBRT)\n      - Random Forest", "metadata": {}, "cell_type": "markdown"}, {"source": "**Deep Learning**\n\nPart of the machine learning field of learning representations of data. \n\nExceptional effective at learning patterns. \n\nUtilizes learning algorithms that derive meaning out of data by using a hierarchy of multiple layers that mimic the neural networks of our brain. \n\nIf you provide the system tons of information, it begins to understand it and respond in useful ways.", "metadata": {}, "cell_type": "markdown"}, {"source": "**AI**\n\n- Artificial Narrow Intelligence (ANI): Machine intelligence that equals or exceeds human intelligence or efficiency at a specific task. \n- Artificial General Intelligence (AGI): A machine with the ability to apply intelligence to any problem, rather than just one specific problem (human-level intelligence). \n- Artificial Superintelligence (ASI): An intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.", "metadata": {}, "cell_type": "markdown"}, {"source": "### 03-02 Regression", "metadata": {}, "cell_type": "markdown"}, {"source": "***Parametric Machine Learning Algorithms***\n\nThe algorithms involve two steps:\n\n- Select a form for the function.\n- Learn the coefficients for the function from the training data.\n\nAssuming the functional form of a line greatly simplifies the learning process. Now, all we need to do is estimate the coefficients of the line equation and we have a predictive model for the problem.\n\nExamples of parametric machine learning algorithms include:\n\n- Logistic Regression\n- Linear Discriminant Analysis\n- Perceptron\n- Naive Bayes\n- Simple Neural Networks\n\nBenefits of Parametric Machine Learning Algorithms:\n\n- Simpler: These methods are easier to understand and interpret results.\n- Speed: Parametric models are very fast to learn from data.\n- Less Data: They do not require as much training data and can work well even if the fit to the data is not perfect.\n\nLimitations of Parametric Machine Learning Algorithms:\n\n- Constrained: By choosing a functional form these methods are highly constrained to the specified form.\n- Limited Complexity: The methods are more suited to simpler problems.\n- Poor Fit: In practice the methods are unlikely to match the underlying mapping function.\n\n***Nonparametric Machine Learning Algorithms***\n\nNonparametric methods seek to best fit the training data in constructing the mapping function, whilst maintaining some ability to generalize to unseen data. As such, they are able to fit a large number of functional forms.\n\nExamples of popular nonparametric machine learning algorithms are:\n\n- k-Nearest Neighbors\n- Decision Trees like CART and C4.5\n- Support Vector Machines\n\nBenefits of Nonparametric Machine Learning Algorithms:\n\n- Flexibility: Capable of fitting a large number of functional forms.\n- Power: No assumptions (or weak assumptions) about the underlying function.\n- Performance: Can result in higher performance models for prediction.\n\nLimitations of Nonparametric Machine Learning Algorithms:\n\n- More data: Require a lot more training data to estimate the mapping function.\n- Slower: A lot slower to train as they often have far more parameters to train.\n- Overfitting: More of a risk to overfit the training data and it is harder to explain why specific predictions are made.", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "*Regression (more examples):*\n- Lasso, Ridge regression (Regularized Linear Regression)\n- Kernel Regression\n- Regression Trees, Splines, Wavelet estimators, \u00e2\u0080\u00a6\n\n*Nearest Neighbors*\nThe principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as non-generalizing machine learning methods, since they simply \u00e2\u0080\u009cremember\u00e2\u0080\u009d all of its training data.\n\n\n*Kernel regression* is a method for nonlinear regression in which the target value for a test point is estimated using a weighted average of the surrounding training samples. The weights are typically obtained by applying a distance-based kernel function to each of the samples, which presumes the existence of a well-defined distance metric.", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"outputs": [{"metadata": {}, "data": {"text/plain": "array([ 0.2,  1.3])"}, "output_type": "execute_result", "execution_count": 29}], "source": "# KNN (K- Nearest Neighbors)\nX = [[0], [1.5], [2], [3.5], [4], [5.5], [6], [7.5], [8], [9.5], [10], [11.5], [12], [1.5], [14], [15.5]]\ny = [0, 0.2, 1.8, 1, 0.6, 1.8, 2.5, 1.3, 3.9, 4.1, 4.6, 2.7, 5.9, 4.4, 6.8, 7.5]\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X, y) \nneigh.predict([[1.9], [6.9]])", "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 29}, {"outputs": [], "source": "%%R\nlibrary(knn)\nx <- cbind(x_train,y_train)\nfit <- knn(y_train ~ ., data = x, k=3)\nsummary(fit) \npredict_y= predict(fit, x_test)", "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null}, {"source": "*Cross-validation*, sometimes called rotation estimation, is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is a prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (testing dataset). The goal of cross-validation is to define a dataset to \"test\" the model in the training phase (i.e., the validation dataset), in order to limit problems like overfitting, give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem), etc.\n\nOne round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation results are averaged over the rounds.\n\nOne of the main reasons for using cross-validation instead of using the conventional validation (e.g. partitioning the dataset into two sets of 70% for training and 30% for test) is that there is not enough data available to partition it into separate training and test sets without losing significant modeling or testing capability. In these cases, a fair way to properly estimate model prediction performance is to use cross-validation as a powerful general technique.\n\nIn summary, cross-validation combines (averages) measures of fit (prediction error) to derive a more accurate estimate of model prediction performance.\n\n***Training set  \u00e2\u0087\u0092 to fit the parameters***\n\n***Validation set \u00e2\u0087\u0092  to tune the parameters***\n\n***Test set \u00e2\u0087\u0092 to evaluate the performance***\n", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "### 03-03 Assessing a learning algorithm\nIndicators to judge the effectivenessof the model.\n\nThe first is the *root mean square error(RMSE)*.", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"outputs": [], "source": "# Root Mean Squared Error (RMSE)\n%R RMSE <- sqrt(mean((y-y_pred)^2))\n\nfrom sklearn.metrics import mean_squared_error\nRMSE = mean_squared_error(y, y_pred)**0.5", "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null}, {"source": "!!!! For financial data, we don\u00e2\u0080\u0099t want to accidentally look forward in time, so we would only use *roll forward cross validation*. This simply demands that all the training data is before the test data.", "metadata": {}, "cell_type": "markdown"}, {"source": "The second metric for how well an algorithm is working is the *correlation* of the test data and predicted values. Strong correlation, close to \u00c2\u00b11, indicates a good algorithm whereas a weak correlation, close to zero, indicates a poor algorithm. \n\n*Overfitting* is the point at which error for training data is decreasing while error for test data is increasing.", "metadata": {}, "cell_type": "markdown"}, {"source": "### 03-04 Ensemble learners, bagging and boosting", "metadata": {}, "cell_type": "markdown"}, {"source": "In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\n\nCombine Model Predictions Into Ensemble Predictions.\n\n- **Bagging**: building multiple models (typically of the same type) from different subsamples of the training dataset.\n- **Boosting**: building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the chain.\n- **Stacking**: building multiple models (typically of differing types) and supervisor model that learns how to best combine the predictions of the primary models.", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "Two families of ensemble methods are usually distinguished:\n\nIn **averaging methods**, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n\nExamples: Bagging methods, Forests of randomized trees, ...\n    \nBy contrast, in **boosting methods**, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n\nExamples: AdaBoost, Gradient Tree Boosting, ...", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "**Bayesian model combination (BMC)** is an algorithmic correction to Bayesian model averaging (BMA). Instead of sampling each model in the ensemble individually, it samples from the space of possible ensembles (with model weightings drawn randomly from a Dirichlet distribution having uniform parameters). This modification overcomes the tendency of BMA to converge toward giving all of the weight to a single model. Although BMC is somewhat more computationally expensive than BMA, it tends to yield dramatically better results.\n\nAt least three R packages offer Bayesian model averaging tools, including the BMS (an acronym for Bayesian Model Selection) package, the BAS (an acronym for Bayesian Adaptive Sampling) package, and the BMA package.", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "### 03-05 Reinforcement Learning", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "**Reinforcement Learning** is a type of Machine Learning, and thereby also a branch of Artificial Intelligence. It allows machines and software agents to automatically determine the ideal behaviour within a specific context, in order to maximize its performance. Simple reward feedback is required for the agent to learn its behaviour; this is known as the reinforcement signal.", "metadata": {}, "cell_type": "markdown"}, {"source": "Reinforcement learning describes the problem that is how to go about maximizing the reward. In the stock market, the reward is return on trades, and we want to find out\nhow to maximize returns. This problem is complicated by time constraints. The value of future gains diminishes with time, so it\u00e2\u0080\u0099s unreasonable to use an infinite horizon on which\nto base returns. However, optimizing returns over too short a time may limit rewards from seeing a much larger overall gain.", "metadata": {}, "cell_type": "markdown"}, {"source": "A **Markov Decision Process (MDP)** is just like a Markov Chain, except the transition matrix depends on the action taken by the decision maker (agent) at each time step. The agent receives a reward, which depends on the action and the state. The goal is to find a function, called a policy, which specifies which action to take in each state, so as to maximize some function (e.g., the mean or expected discounted sum) of the sequence of rewards. One can formalize this in terms of Bellman's equation, which can be solved iteratively using policy iteration. The unique fixed point of this equation is the optimal policy.\n\nA Markov Decision Process (MDP) model contains:\n- A set of possible world states S\n- A set of possible actions A\n- A real valued reward function R(s,a)\n- A description T of each action\u00e2\u0080\u0099s effects in each state.\n\nMarkov Property: the effects of an action taken in a state depend only on that state and not on the prior history.", "metadata": {}, "cell_type": "markdown"}, {"source": "### 03-06 Q-Learning ", "metadata": {}, "cell_type": "markdown"}, {"source": "**Q-learning** is a **model-free reinforcement learning technique**. \n\nSpecifically, Q-learning can be used to find an optimal action-selection policy for any given (finite) Markov decision process (MDP). It works by learning an action-value function that ultimately gives the expected utility of taking a given action in a given state and following the optimal policy thereafter. A policy is a rule that the agent follows in selecting actions, given the state it is in. When such an action-value function is learned, the optimal policy can be constructed by simply selecting the action with the highest value in each state. \n\nOne of the strengths of Q-learning is that it is able to compare the expected utility of the available actions without requiring a model of the environment. Additionally, Q-learning can handle problems with stochastic transitions and rewards, without requiring any adaptations. It has been proven that for any finite MDP, Q-learning eventually finds an optimal policy, in the sense that the expected value of the total reward return over all successive steps, starting from the current state, is the maximum achievable.", "metadata": {}, "cell_type": "markdown"}, {"source": "Q-Learning doesn\u00e2\u0080\u0099t need to have any sort of model of the transition function T or the reward function R. It builds a table of utility values as the agent interacts with the world. These are the Q values. At each state, the agent can then use the Q values to select the best action. Q-Learning is guaranteed to give an optimal policy, as it is proven to always converge. Q represents the value of taking action a in state s. This value includes the immediate reward for taking action a, and the discounted (future) reward for all optimal future actions after having taken a.\n\nWhat we want to find for a particular state is what policy, \u00ce\u00a0(s) we should take. Using Q values, all we need to do is find the maximum Q value for that state.\n\n\u00ce\u00a0(s) = argmaxa(Q[s, a]) \n\nSo, we go through each action a and see which action has the maximum Q value for state s. Eventually, after learning enough, the agent will converge to the optimal policy, \u00cf\u0080\u00e2\u0088\u0097(s), and optimal Q table, Q\u00e2\u0088\u0097[s, a].\n\n1. Initialize the Q table with small random values\n2. Compute s\n3. Select a\n4. Observe r, s\n5. Update Q\n6. Step forward in time, then repeat from step 2.\n", "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": "\"\"\"\nUpdate Rule\n\nThe formula for computing Q for any state-action pair <s, a>, given an experience tuple <s, a, s', r>, is:\n\nQ'[s, a] = (1 - \u00ce\u00b1) \u00c2\u00b7 Q[s, a] + \u00ce\u00b1 \u00c2\u00b7 (r + \u00ce\u00b3 \u00c2\u00b7 Q[s', argmaxa'(Q[s', a'])])\n\nHere:\n\n- r = R[s, a] is the immediate reward for taking action a in state s,\n- \u00ce\u00b3 \u00e2\u0088\u0088 [0, 1] (gamma) is the discount factor used to progressively reduce the value of future rewards,\n- s' is the resulting next state,\n- argmaxa'(Q[s', a']) is the action that maximizes the Q-value among all possible actions a' from s', and,\n- \u00ce\u00b1 \u00e2\u0088\u0088 [0, 1] (alpha) is the learning rate used to vary the weight given to new experiences compared with past Q-values.\n\"\"\"", "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null}, {"source": "**Exploration**\n\nThe success of a Q-learning algorithm depends on the exploration of the state-action space. If you only explore a small subset of it, you might not find the best policies. One way\nto ensure that you explore as much as possible is to introduce randomness into selecting actions during the learning phase. So basically, you see first whether you want to take the action with the maximal Q value or choose a random action, then if you take a random action, each action gets a probability which decreases over subsequent iterations.\n", "metadata": {}, "cell_type": "markdown"}, {"source": "**Q-Learning for Trading**\n\nNow that we know what Q-learning is, we need to figure out how to apply it to the context of trading. That means that we need to define what state, action, and reward mean. \n\nActions are straightforward, as there are basically three of them:\n\u00e2\u0080\u00a2 BUY\n\u00e2\u0080\u00a2 SELL\n\u00e2\u0080\u00a2 NOTHING\n\nOur rewards can be daily returns or cumulative returns after a trade cycle (buy\u00e2\u0086\u0092sell). However, using daily returns will allow the agent to converge on a Q value more quickly, because if it waited until a sell, then it would have to look at all of the actions backwards until the buy to get that reward.\n\nNow, we just need to figure out how to determine state. Some good factors to determine state are:\n\n- Adjusted Close/Simple Moving Average\n- Bollinger Band value\n- P/E ratio\n- Holding stock (whether or not we\u00e2\u0080\u0099re holding the stock)\n- Return since entry\n\nOur state must be a single number so we can look it up in the table easily. To make it simpler, we\u00e2\u0080\u0099ll confine the state to be an integer, which means we need to discretize each factor and then combine them into an overall state. Our state space is discrete, so the combined value is the overall state.", "metadata": {}, "cell_type": "markdown"}, {"source": "**Discretization**\n\nIn statistics and machine learning, discretization refers to the process of converting or partitioning continuous attributes, features or variables to discretized or nominal attributes/features/variables/intervals. This can be useful when creating probability mass functions \u00e2\u0080\u0093 formally, in density estimation. It is a form of discretization in general and also of binning, as in making a histogram. Whenever continuous data is discretized, there is always some amount of discretization error. The goal is to reduce the amount to a level considered negligible for the modeling purposes at hand.\n\nTo discretize, what we do is take the data for a factor over its range, then divide it into n bins. Then we find the threshold by iterating over the data by the step size\nand taking the value at each position.\n", "metadata": {}, "cell_type": "markdown"}, {"source": "One main problem with Q-Learning is that it takes a lot of experience tuples to converge to the optimal Q value. This means the agent has to take many real interactions with the world (execute trades) to learn.", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "**Advantages**\n\nThe main advantage of a model-free approach like Q-Learning over model-based techniques is that it can easily be applied to domains where all states and/or transitions are not fully defined.\n\nAs a result, we do not need additional data structures to store transitions T(s, a, s') or rewards R(s, a).\n\nAlso, the Q-value for any state-action pair takes into account future rewards. Thus, it encodes both the best possible value of a state (maxa Q(s, a)) as well as the best policy in terms of the action that should be taken (argmaxa Q(s, a)).\n\n**Issues**\n\nThe biggest challenge is that the reward (e.g. for buying a stock) often comes in the future - representing that properly requires look-ahead and careful weighting.\n\nAnother problem is that taking random actions (such as trades) just to learn a good strategy is not really feasible (you'll end up losing a lot of money!).\n\nIn the next lesson, we will discuss an algorithm that tries to address this second problem by simulating the effect of actions based on historical data.", "metadata": {}, "cell_type": "markdown"}, {"source": "### 03-07 Dyna", "metadata": {}, "cell_type": "markdown"}, {"source": "Dyna-Q is a simple architecture integrating the major functions needed in an on-line planning agent. ", "metadata": {}, "cell_type": "markdown"}, {"source": "Dyna-Q includes the processes: planning, acting, model-learning, and direct RL (all occurring continually). \n\n- The planning method is the random-sample one-step tabular Q-planning method. \n\n- The direct RL method is one-step tabular Q-learning. \n\n- The model-learning method is also table-based and assumes the world is deterministic.", "metadata": {}, "cell_type": "markdown"}, {"source": "Quiz: How To Evaluate T?\n\n## $\\frac{T_c\\left[s,a,s'\\right]}{\\Sigma_iT_c\\left[s,a,i\\right]}$", "metadata": {}, "cell_type": "markdown"}, {"source": "R'[s, a] = (1 \u00e2\u0088\u0092 \u00ce\u00b1)R[s, a] + \u00ce\u00b1r", "metadata": {}, "cell_type": "markdown"}, {"source": "How Dyna-Q works:\n\n- Q learning\n\n  - init Q table\n  - observe S\n  - execute a, observe (s,r)\n  - update Q with (s, a, s', ri)\n  \n- Update model \u001a\n\n  - update T\u00e2\u0080\u0099[s,a,s\u00e2\u0080\u0099]\n  - update R\u00e2\u0080\u0099[s,a]\n  \n- Dyna Q\n\n  - s = random\n  - a = random\n  - s\u00e2\u0080\u0099 = infer from T\n  - r = R[s,a]\n  - update Q with new experience tuple\n  - repeat many times (\u00e2\u0088\u00bc 100 \u00e2\u0088\u0092 200)\n", "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": "", "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null}], "nbformat": 4}