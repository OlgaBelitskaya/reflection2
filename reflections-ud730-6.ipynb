{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &#x1F4D1; &nbsp; <span style=\"color:red\"> Reflections. Deep Learning. Lesson 6</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   &#x1F916; &nbsp; <span style=\"color:red\">Links</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some Lesser-Known Deep Learning Libraries\n",
    "\n",
    "http://blog.paralleldots.com/technology/deep-learning/lesser-known-deep-learning-libraries/?utm_content=buffer02038&utm_medium=social&utm_source=facebook.com&utm_campaign=buffer\n",
    "\n",
    "Backpropagation Algorithm http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm\n",
    "\n",
    "Keras.Guide to the Sequential model https://keras.io/getting-started/sequential-model-guide/\n",
    "\n",
    "TensorFlow for Deep Learning Research http://web.stanford.edu/class/cs20si/lectures/\n",
    "\n",
    "Recurrent Neural Networks in Tensorflow II http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   &#x1F916; &nbsp; <span style=\"color:red\">Libraries</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow.examples.tutorials.mnist as mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier, BernoulliRBM\n",
    "from sklearn import linear_model, datasets, metrics\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.layers import Embedding, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   &#x1F916; &nbsp; <span style=\"color:red\"> Data </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist_data = mnist.input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = mnist_data.train.images\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = mnist_data.train.labels\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels2 = np.array([ np.where(r==1)[0][0] for r in train_labels])\n",
    "train_labels2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images = mnist_data.test.images\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = mnist_data.test.labels\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels2 = np.array([ np.where(r==1)[0][0] for r in test_labels])\n",
    "test_labels2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' \n",
    "                                % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' \n",
    "                               % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                    dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, columns: 784\n",
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "load_mnist('')\n",
    "X_train, y_train = load_mnist('', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
    "\n",
    "X_test, y_test = load_mnist('', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "X, y = digits.data, digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_x_train = np.random.random((100, 100, 100, 3))\n",
    "r_y_train = ks.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "r_x_test = np.random.random((20, 100, 100, 3))\n",
    "r_y_test = ks.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 100, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   &#x1F916; &nbsp; <span style=\"color:red\"> Software and Tools </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network models (supervised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60156824\n",
      "Iteration 2, loss = 0.27297370\n",
      "Iteration 3, loss = 0.20647665\n",
      "Iteration 4, loss = 0.17051029\n",
      "Iteration 5, loss = 0.14766734\n",
      "Iteration 6, loss = 0.13066736\n",
      "Iteration 7, loss = 0.11707463\n",
      "Iteration 8, loss = 0.10379596\n",
      "Iteration 9, loss = 0.09576563\n",
      "Iteration 10, loss = 0.08580545\n",
      "Iteration 11, loss = 0.07876979\n",
      "Iteration 12, loss = 0.07143936\n",
      "Iteration 13, loss = 0.06422787\n",
      "Iteration 14, loss = 0.05865670\n",
      "Iteration 15, loss = 0.05444742\n",
      "Iteration 16, loss = 0.05104569\n",
      "Iteration 17, loss = 0.04502452\n",
      "Iteration 18, loss = 0.04091311\n",
      "Iteration 19, loss = 0.03783826\n",
      "Iteration 20, loss = 0.03472363\n",
      "Iteration 21, loss = 0.03240190\n",
      "Iteration 22, loss = 0.02882075\n",
      "Iteration 23, loss = 0.02590460\n",
      "Iteration 24, loss = 0.02426535\n",
      "Iteration 25, loss = 0.02127423\n",
      "Iteration 26, loss = 0.01985075\n",
      "Iteration 27, loss = 0.01848932\n",
      "Iteration 28, loss = 0.01739222\n",
      "Iteration 29, loss = 0.01442560\n",
      "Iteration 30, loss = 0.01318563\n",
      "Iteration 31, loss = 0.01165906\n",
      "Iteration 32, loss = 0.01081314\n",
      "Iteration 33, loss = 0.01023427\n",
      "Iteration 34, loss = 0.00945244\n",
      "Iteration 35, loss = 0.00863456\n",
      "Iteration 36, loss = 0.00807347\n",
      "Iteration 37, loss = 0.00751514\n",
      "Iteration 38, loss = 0.00705181\n",
      "Iteration 39, loss = 0.00686093\n",
      "Iteration 40, loss = 0.00640312\n",
      "Iteration 41, loss = 0.00601985\n",
      "Iteration 42, loss = 0.00553678\n",
      "Iteration 43, loss = 0.00528085\n",
      "Iteration 44, loss = 0.00513325\n",
      "Iteration 45, loss = 0.00490632\n",
      "Iteration 46, loss = 0.00479794\n",
      "Iteration 47, loss = 0.00455585\n",
      "Iteration 48, loss = 0.00440715\n",
      "Iteration 49, loss = 0.00425342\n",
      "Iteration 50, loss = 0.00409615\n",
      "Iteration 51, loss = 0.00399118\n",
      "Iteration 52, loss = 0.00390171\n",
      "Iteration 53, loss = 0.00374273\n",
      "Iteration 54, loss = 0.00366609\n",
      "Iteration 55, loss = 0.00358211\n",
      "Iteration 56, loss = 0.00347165\n",
      "Iteration 57, loss = 0.00337382\n",
      "Iteration 58, loss = 0.00332926\n",
      "Iteration 59, loss = 0.00322446\n",
      "Iteration 60, loss = 0.00316987\n",
      "Iteration 61, loss = 0.00313216\n",
      "Iteration 62, loss = 0.00306905\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=70, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "clf.fit(mnist_data.train.images, mnist_data.train.labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_predict = clf.predict(mnist_data.train.images)\n",
    "test_predict = clf.predict(mnist_data.test.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train score: %f\" % clf.score(mnist_data.train.images, mnist_data.train.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.955200\n"
     ]
    }
   ],
   "source": [
    "print(\"Test score: %f\" % clf.score(mnist_data.test.images, mnist_data.test.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Bernoulli Restricted Boltzmann machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "brbm = BernoulliRBM(random_state=0, verbose=True)\n",
    "classifier = Pipeline(steps=[('brbm', brbm), ('logistic', logistic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brbm.learning_rate = 0.01\n",
    "brbm.n_iter = 50\n",
    "brbm.n_components = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -109.53, time = 14.53s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -95.96, time = 19.09s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -90.01, time = 18.33s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -86.53, time = 17.15s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -83.10, time = 17.12s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -81.59, time = 17.82s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -81.04, time = 18.06s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -79.26, time = 16.51s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -77.85, time = 16.18s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -78.41, time = 17.42s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -77.36, time = 16.82s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -76.75, time = 17.12s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -76.29, time = 16.93s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -76.38, time = 17.34s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -75.66, time = 16.28s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -74.91, time = 16.38s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -74.85, time = 15.90s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -74.02, time = 16.17s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -73.18, time = 16.61s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -73.06, time = 17.03s\n",
      "[BernoulliRBM] Iteration 21, pseudo-likelihood = -72.88, time = 17.33s\n",
      "[BernoulliRBM] Iteration 22, pseudo-likelihood = -73.35, time = 16.32s\n",
      "[BernoulliRBM] Iteration 23, pseudo-likelihood = -73.61, time = 16.07s\n",
      "[BernoulliRBM] Iteration 24, pseudo-likelihood = -72.70, time = 15.79s\n",
      "[BernoulliRBM] Iteration 25, pseudo-likelihood = -72.58, time = 15.66s\n",
      "[BernoulliRBM] Iteration 26, pseudo-likelihood = -72.56, time = 16.65s\n",
      "[BernoulliRBM] Iteration 27, pseudo-likelihood = -72.17, time = 16.92s\n",
      "[BernoulliRBM] Iteration 28, pseudo-likelihood = -72.06, time = 17.14s\n",
      "[BernoulliRBM] Iteration 29, pseudo-likelihood = -71.65, time = 16.91s\n",
      "[BernoulliRBM] Iteration 30, pseudo-likelihood = -71.74, time = 16.43s\n",
      "[BernoulliRBM] Iteration 31, pseudo-likelihood = -71.65, time = 16.21s\n",
      "[BernoulliRBM] Iteration 32, pseudo-likelihood = -71.14, time = 16.38s\n",
      "[BernoulliRBM] Iteration 33, pseudo-likelihood = -71.38, time = 16.64s\n",
      "[BernoulliRBM] Iteration 34, pseudo-likelihood = -70.88, time = 15.72s\n",
      "[BernoulliRBM] Iteration 35, pseudo-likelihood = -70.97, time = 15.88s\n",
      "[BernoulliRBM] Iteration 36, pseudo-likelihood = -71.26, time = 15.60s\n",
      "[BernoulliRBM] Iteration 37, pseudo-likelihood = -71.19, time = 15.74s\n",
      "[BernoulliRBM] Iteration 38, pseudo-likelihood = -71.06, time = 16.05s\n",
      "[BernoulliRBM] Iteration 39, pseudo-likelihood = -71.03, time = 15.81s\n",
      "[BernoulliRBM] Iteration 40, pseudo-likelihood = -71.37, time = 15.86s\n",
      "[BernoulliRBM] Iteration 41, pseudo-likelihood = -69.22, time = 15.57s\n",
      "[BernoulliRBM] Iteration 42, pseudo-likelihood = -71.07, time = 15.36s\n",
      "[BernoulliRBM] Iteration 43, pseudo-likelihood = -70.54, time = 15.81s\n",
      "[BernoulliRBM] Iteration 44, pseudo-likelihood = -70.39, time = 16.26s\n",
      "[BernoulliRBM] Iteration 45, pseudo-likelihood = -70.19, time = 15.88s\n",
      "[BernoulliRBM] Iteration 46, pseudo-likelihood = -70.08, time = 15.49s\n",
      "[BernoulliRBM] Iteration 47, pseudo-likelihood = -70.97, time = 15.54s\n",
      "[BernoulliRBM] Iteration 48, pseudo-likelihood = -70.22, time = 16.58s\n",
      "[BernoulliRBM] Iteration 49, pseudo-likelihood = -70.43, time = 16.98s\n",
      "[BernoulliRBM] Iteration 50, pseudo-likelihood = -70.49, time = 16.75s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('brbm', BernoulliRBM(batch_size=10, learning_rate=0.01, n_components=128, n_iter=50,\n",
       "       random_state=0, verbose=True)), ('logistic', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(train_images, train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using RBM features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.97       980\n",
      "          1       0.97      0.98      0.98      1135\n",
      "          2       0.95      0.92      0.94      1032\n",
      "          3       0.93      0.93      0.93      1010\n",
      "          4       0.95      0.95      0.95       982\n",
      "          5       0.93      0.91      0.92       892\n",
      "          6       0.96      0.97      0.96       958\n",
      "          7       0.95      0.93      0.94      1028\n",
      "          8       0.92      0.93      0.93       974\n",
      "          9       0.92      0.92      0.92      1009\n",
      "\n",
      "avg / total       0.95      0.95      0.95     10000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (metrics.classification_report(test_labels2,\n",
    "                                                         classifier.predict(test_images))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_classifier = linear_model.LogisticRegression()\n",
    "logistic_classifier.fit(train_images, train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using raw pixel features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.96       980\n",
      "          1       0.96      0.98      0.97      1135\n",
      "          2       0.94      0.89      0.91      1032\n",
      "          3       0.90      0.91      0.90      1010\n",
      "          4       0.92      0.93      0.92       982\n",
      "          5       0.89      0.86      0.88       892\n",
      "          6       0.94      0.95      0.94       958\n",
      "          7       0.93      0.92      0.93      1028\n",
      "          8       0.88      0.87      0.87       974\n",
      "          9       0.90      0.89      0.90      1009\n",
      "\n",
      "avg / total       0.92      0.92      0.92     10000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic regression using raw pixel features:\\n%s\\n\" % (metrics.classification_report(test_labels2,\n",
    "                                                               logistic_classifier.predict(test_images))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- tf.TextLineReader\n",
    "  - Outputs the lines of a file delimited by newlines\n",
    "  - E.g. text files, CSV files\n",
    "\n",
    "- tf.FixedLengthRecordReader\n",
    "  - Outputs the entire file when all files have same fixed lengths\n",
    "  - E.g. each MNIST file has 28 x 28 pixels, CIFAR-10 32 x 32 x 3\n",
    "\n",
    "- tf.WholeFileReader\n",
    "  - Outputs the entire file content\n",
    "\n",
    "- tf.TFRecordReader\n",
    "  - Reads samples from TensorFlow's own binary format (TFRecord)\n",
    "  - tf.ReaderBase\n",
    "  - Allows you to create your own readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 3s - loss: 1.0971 - acc: 0.7099     \n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 3s - loss: 0.5557 - acc: 0.8795     \n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 3s - loss: 0.5034 - acc: 0.8903     \n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 3s - loss: 0.4920 - acc: 0.8907     \n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 3s - loss: 0.4611 - acc: 0.8965     \n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.4412 - acc: 0.8990     \n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.4399 - acc: 0.8997     \n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.4296 - acc: 0.9022     \n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.4203 - acc: 0.9037     \n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.4078 - acc: 0.9082     \n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.4041 - acc: 0.9054     \n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.3977 - acc: 0.9081     \n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.3906 - acc: 0.9093     \n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.3882 - acc: 0.9087     \n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.3863 - acc: 0.9101     \n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.3817 - acc: 0.9095     \n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.3791 - acc: 0.9123     \n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.3782 - acc: 0.9095     \n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.3679 - acc: 0.9131     \n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.3627 - acc: 0.9147     \n",
      " 8064/10000 [=======================>......] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "mlp_model.add(Dense(64, activation='relu', input_dim=784))\n",
    "mlp_model.add(Dropout(0.5))\n",
    "mlp_model.add(Dense(64, activation='relu'))\n",
    "mlp_model.add(Dropout(0.5))\n",
    "mlp_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "mlp_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "mlp_model.fit(train_images, train_labels, epochs=20, batch_size=128)\n",
    "\n",
    "mlp_score = mlp_model.evaluate(test_images, test_labels, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17249117684364318, 0.9577]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks.backend.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 10s - loss: 2.3913 - acc: 0.0900    \n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 9s - loss: 2.3126 - acc: 0.1100     \n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 10s - loss: 2.2969 - acc: 0.1700    \n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 71s - loss: 2.2925 - acc: 0.1100    \n",
      "Epoch 5/20\n",
      " 64/100 [==================>...........] - ETA: 3s - loss: 2.2607 - acc: 0.1719"
     ]
    }
   ],
   "source": [
    "x_train = np.random.random((100, 100, 100, 3))\n",
    "y_train = ks.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "x_test = np.random.random((20, 100, 100, 3))\n",
    "y_test = ks.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=20)\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
