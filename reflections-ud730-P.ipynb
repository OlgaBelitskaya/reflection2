{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &#x1F4D1; &nbsp; <span style=\"color:red\"> Reflections. Deep Learning. Project</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   &#x1F916; &nbsp; <span style=\"color:red\">Links</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "CS231n Convolutional Neural Networks for Visual Recognition\n",
    "\n",
    "- Optimization http://cs231n.github.io/optimization-1/\n",
    "- Intuitive understanding of backpropagation http://cs231n.github.io/optimization-2/\n",
    "- Neural Network architectures http://cs231n.github.io/neural-networks-1/\n",
    "- Data Preprocessing, Loss Functions http://cs231n.github.io/neural-networks-2/\n",
    "- Gradient Check, Sanity Check http://cs231n.github.io/neural-networks-3/\n",
    "- Convolutional Neural Networks http://cs231n.github.io/convolutional-networks/\n",
    "- Visualizing Convolutional Neural Networks http://cs231n.github.io/understanding-cnn/\n",
    "- Transfer Learning http://cs231n.github.io/transfer-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   &#x1F916; &nbsp; <span style=\"color:red\">Libraries</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   &#x1F916; &nbsp; <span style=\"color:red\"> Build a Live Camera App </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Step 1: Design and test a model architecture that can identify sequences of digits in an image.\n",
    "Design and implement a deep learning model that learns to recognize sequences of digits. Train it using synthetic data first (recommended) or directly use real-world data (see step 2).\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "Your model can be derived from a deep neural net or a convolutional network.\n",
    "You could experiment sharing or not the weights between the softmax classifiers.\n",
    "You can also use a recurrent network in your deep neural net to replace the classification layers and directly emit the sequence of digits one-at-a-time.\n",
    "To help you develop your model, the simplest path is likely to generate a synthetic dataset by concatenating character images from notMNIST or MNIST. This can provide you with a quick way to run experiments. (Or you can go directly to the real-world dataset of Step 2.)\n",
    "\n",
    "In order to produce a synthetic sequence of digits for testing, you can for example limit yourself to sequences up to five digits, and use five classifiers on top of your deep network. You would have to incorporate an additional ‘blank’ character to account for shorter number sequences.\n",
    "\n",
    "Here is for example a published baseline model on this problem (video).\n",
    "\n",
    "Report:\n",
    "\n",
    "What approach did you take in coming up with a solution to this problem?\n",
    "What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)\n",
    "How did you train your model? Did you generate a synthetic dataset (if so, explain how)?\n",
    "Step 2: Train a model on a realistic dataset.\n",
    "Once you have settled on a good architecture, you can train your model on real data. In particular, the SVHN dataset is a good large scale dataset collected from house numbers in Google Street View. Training on this more challenging dataset, where the digits are not neatly lined-up and have various skews, fonts and colors, likely means you have to do some hyperparameter exploration to do well.\n",
    "\n",
    "Report:\n",
    "\n",
    "How does your model perform on a realistic dataset?\n",
    "What changes did you have to make, if any?\n",
    "Step 3 (optional): Put the model into an Android app.\n",
    "Do this step only if you have access to an Android device. If you don’t, you may either:\n",
    "\n",
    "take pictures of numbers that you find around you, and run them through your classifier on your computer to produce example results, or,\n",
    "use OpenCV / SimpleCV / Pygame to capture live images from a webcam.\n",
    "Loading a TensorFlow model into a camera app on Android is demonstrated in the TensorFlow Android demo app, which you can simply modify.\n",
    "\n",
    "Report:\n",
    "\n",
    "Is your model able to perform equally well on captured pictures or a live camera stream?\n",
    "Document how you built the interface to your model.\n",
    "Step 4: Explore!\n",
    "There are many things you can do once you have the basic classifier in place. One example would be to also localize where the numbers are on the image. The SVHN dataset provides bounding boxes that you can tune to train a localizer. Simply training a regression loss to the coordinates of the bounding box is one way to get decent localization.\n",
    "\n",
    "Once you have the data localized, you can for example try turn it into an augmented reality app by overlaying your answer on the image like the Word Lens app does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
